{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "from torch.utils.data import dataloader\n",
    "import utils\n",
    "from arguments import parse_args\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import wandb\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from sampling import DelaunayTriangulationBlur\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numba as nb\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "from IPython.display import clear_output as clear\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class TrainDataset_PictureOnly(data.Dataset):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.root_path = os.path.join(args.data_root, 'train')\n",
    "        self.img_list = os.listdir(self.root_path)\n",
    "        try:\n",
    "            self.img_list.remove('.DS_Store')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_path, self.img_list[idx])\n",
    "        img_raw = Image.open(img_path)\n",
    "        img = img_raw.resize((256, 256))\n",
    "        img = ToTensor()(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args([\"--alg\", \"MAE\", \"--description\", \"mae_pretrain_piconly\", \"--lr\", \"1e-3\", \"--epoch\", \"100\", '--data_root', 'data/celeba', '--batch_size', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"MAE-toy-modified-celebA-1000-12131124\", entity=\"purewhite2019\")\n",
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 1,\n",
    "  \"seed\" : 31415926\n",
    "}\n",
    "\n",
    "# Training Preparation\n",
    "utils.set_seed_everywhere(31415926)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "train_dataset = TrainDataset_PictureOnly(args)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=args.batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=args.num_workers,\n",
    "                                pin_memory=True,\n",
    "                                drop_last=True)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# loss_function = lambda img_gt, img_pred : -utils.psnr(img_gt, img_pred) - utils.ssim(img_gt, img_pred)\n",
    "\n",
    "from models.vit import ViT\n",
    "from models.mae import MAE\n",
    "\n",
    "img_size, patch_size = (256, 256), (16, 16)\n",
    "\n",
    "# encoder = ViT(img_size, patch_size, depth=12, dim=768, mlp_dim=3072, num_heads=12) # ViT-B/16\n",
    "# encoder = ViT(img_size, patch_size, depth=24, dim=1024, mlp_dim=4096, num_heads=16) # ViT-L/16 (Default in MAE paper)\n",
    "# encoder = ViT(img_size, patch_size, depth=32, dim=1280, mlp_dim=5120, num_heads=16) # ViT-H/16\n",
    "encoder = ViT(img_size, patch_size, depth=6, dim=512, mlp_dim=1024, num_heads=8) # Simple\n",
    "\n",
    "# model = MAE(encoder, decoder_depth=8, decoder_dim=512, mask_ratio=0.75) # (Default in MAE paper)\n",
    "model = MAE(encoder, decoder_depth=6, decoder_dim=512, mask_ratio=0.75)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.RAdam(params=model.parameters(),lr=args.lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=1000,gamma = 0.5)\n",
    "\n",
    "e = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Start training\n",
    "lr_list = []\n",
    "model.train()\n",
    "wandb.watch(model)\n",
    "\n",
    "# for _ in range(800): # Default setting in MAE paper\n",
    "for _ in range(args.epoch):\n",
    "    idx = random.randint(0, len(train_dataset)-1)\n",
    "    img = train_dataset[idx].unsqueeze(0).to(device)\n",
    "    utils.save_model(model, e, args)\n",
    "    clear()\n",
    "    recons_img, patches_to_img = model.predict(img)\n",
    "    recons_img = recons_img[0].permute(1, 2, 0).cpu().numpy()\n",
    "    patches_to_img = patches_to_img[0].permute(1, 2, 0).cpu().numpy()\n",
    "    img_gt = img[0].permute(1, 2, 0).cpu().numpy()\n",
    "    utils.show_gt_and_pred(img_hr=patches_to_img, img_lr=recons_img, pred_hr=img_gt, figsize=(30, 30))\n",
    "    \n",
    "    loop = tqdm.tqdm(train_loader)\n",
    "    for idx, img in enumerate(loop):\n",
    "        \n",
    "        img = img.to(device)\n",
    "        loss = model(img)\n",
    "            \n",
    "        wandb.log({\"loss\": loss})\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        \n",
    "        loop.set_description(f\"epoch: {e} | iter: {idx}/{len(train_dataset)} | loss: {loss.item()}\")\n",
    "    e += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0a14ab1b1970d62b335c99762f5a19019209fb6618cd6df77ae1340d6621ffa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('cs231n': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
